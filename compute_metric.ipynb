{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.models.optical_flow import raft_large, Raft_Large_Weights\n",
    "\n",
    "# Path load\n",
    "root = 'results/sem_cyclegan_6b/test_latest/images'\n",
    "outputs = sorted([os.path.join(root, output) for output in os.listdir(root) if ('fake_B' in output)])\n",
    "noises = sorted([os.path.join(root, output) for output in os.listdir(root) if ('real_A' in output)])\n",
    "gts = sorted([os.path.join(root, output) for output in os.listdir(root) if ('real_B' in output)])\n",
    "\n",
    "# Device setting\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Save option\n",
    "save_folder = 'results/aligned'\n",
    "save_image = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocess\n",
    "def preprocess(batch):\n",
    "    transforms = T.Compose(\n",
    "        [   \n",
    "            T.ToTensor(),\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize(mean=0.5, std=0.5),  # map [0, 1] into [-1, 1]\n",
    "            T.Resize(size=(520, 960), antialias=True), # Resize to a size supported by RAFT\n",
    "        ]\n",
    "    )\n",
    "    return transforms(batch)\n",
    "\n",
    "\n",
    "# Data resize to original size\n",
    "def resize(batch):\n",
    "    transforms = T.Compose(\n",
    "        [   \n",
    "            T.Resize(size=(704, 704), antialias=True), # Resize to original size\n",
    "        ]\n",
    "    )\n",
    "    return transforms(batch)\n",
    "\n",
    "\n",
    "# Warp function\n",
    "def warp_feature(x, flow):\n",
    "    \"\"\"\n",
    "    x: (B, C, H, W)\n",
    "    flow: (B, 2, H, W) - (u, v) shape optical flow\n",
    "        Convert the flow into a normalized grid with values in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.size()\n",
    "    # generate index grid\n",
    "    # normalized coords: -1 ~ 1 range\n",
    "    y, x_ = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, H, device=x.device),\n",
    "        torch.linspace(-1, 1, W, device=x.device),\n",
    "        indexing='ij',\n",
    "    )\n",
    "    # (H, W) -> (B, H, W)\n",
    "    grid = torch.stack((x_, y), dim=-1).unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "    # Separate the flow into its horizontal (u) and vertical (v) components.\n",
    "    flow_u = flow[:, 0, :, :]  # (B, H, W)\n",
    "    flow_v = flow[:, 1, :, :]  # (B, H, W)\n",
    "\n",
    "    # Assume that flow_u and flow_v are normalized to the range [â€“1, 1].\n",
    "    grid[:, :, :, 0] += flow_u\n",
    "    grid[:, :, :, 1] += flow_v\n",
    "\n",
    "    return F.grid_sample(x, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "\n",
    "def flow_normalize(x):\n",
    "    B, _, _, _ = x.shape\n",
    "    max_ = torch.tensor([[[520]], [[960]]], device='cuda:0').repeat(B, 1, 1, 1)\n",
    "    x_norm = 2 * (-x + max_) / (2 * max_ + 1e-8) - 1\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total L1 loss with an align method : \n",
      " tensor(0.0701)\n"
     ]
    }
   ],
   "source": [
    "# L1 loss with an align method\n",
    "# Model load\n",
    "model = raft_large(weights=Raft_Large_Weights.C_T_SKHT_V2, progress=False).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "# Compute optical flow and warp images\n",
    "total_loss = []\n",
    "for i in range(len(gts)):\n",
    "    # RAFT only supports color images so repeat the gray image across three channels.\n",
    "    outputs_batch = preprocess(Image.open(outputs[i])).repeat(1, 3, 1, 1).to(device)\n",
    "    gts_batch = preprocess(Image.open(gts[i])).repeat(1, 3, 1, 1).to(device)\n",
    "    mask = torch.ones_like(outputs_batch)\n",
    "    with torch.no_grad():\n",
    "        list_of_flows = model(outputs_batch, gts_batch)\n",
    "    flow = list_of_flows[-1]\n",
    "    flow_normalized = flow_normalize(flow)\n",
    "    x = torch.mean(warp_feature(outputs_batch, flow_normalized), dim=1)\n",
    "    mask = torch.mean(warp_feature(mask, flow_normalized), dim=1)\n",
    "    gts_batch_ = torch.mean(gts_batch, dim=1) * (mask > 0.5)\n",
    "    total_loss.append(F.l1_loss(resize((0.5 + gts_batch_ * 0.5).clamp(0,1)), resize((0.5 + 0.5 * x).clamp(0, 1))))\n",
    "\n",
    "    if save_image:\n",
    "        x_ = ((x.squeeze(0) * 0.5 + 0.5).clamp(0, 1) * 255).detach().to(torch.uint8).cpu().numpy()\n",
    "        outputs_batch = ((torch.mean(outputs_batch.squeeze(0), dim=0) * 0.5 + 0.5).clamp(0, 1) * 255).detach().to(torch.uint8).cpu().numpy()\n",
    "        gts_batch_ = ((torch.mean(gts_batch.squeeze(0), dim=0) * 0.5 + 0.5).clamp(0, 1) * 255).detach().to(torch.uint8).cpu().numpy()\n",
    "        x_ = cv2.resize(x_, (704, 704), interpolation=cv2.INTER_LINEAR)\n",
    "        noise = np.array(Image.open(noises[i]))\n",
    "        gts_batch_ = cv2.resize(gts_batch_, (704, 704), interpolation=cv2.INTER_LINEAR)\n",
    "        outputs_batch = cv2.resize(outputs_batch, (704, 704), interpolation=cv2.INTER_LINEAR)\n",
    "        images = [noise, outputs_batch, x_, gts_batch_]\n",
    "        labels = ['Noise Input', 'Output', 'Aligned Output', 'Ground Truth']\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "        for ax, img, lbl in zip(axes.flatten(), images, labels):\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(lbl, fontsize=12)\n",
    "            ax.axis('off')\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        plt.tight_layout(pad=1.0)\n",
    "        plt.savefig(f'{save_folder}/aligned_{i:04d}.pdf', dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('Total L1 loss with an align method : \\n', torch.mean(torch.tensor(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total L1 loss without an align method : \n",
      " tensor(0.1395)\n"
     ]
    }
   ],
   "source": [
    "# L1 loss without an align method\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "total_loss = []\n",
    "for i in range(len(gts)):\n",
    "    outputs_batch = to_tensor(Image.open(outputs[i]))\n",
    "    gts_batch = to_tensor(Image.open(gts[i]))\n",
    "    total_loss.append(F.l1_loss(gts_batch, outputs_batch))\n",
    "\n",
    "print('Total L1 loss without an align method : \\n', torch.mean(torch.tensor(total_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hynix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
